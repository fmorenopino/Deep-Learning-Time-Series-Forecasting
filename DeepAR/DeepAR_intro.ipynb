{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepAR Time Series Forecasting (Training and Testing)\n",
    "\n",
    "This notebook demonstrates how to train and test your DeepAR time series forecasting model using the code provided in your `main.py`, `evaluate.py`, `dataloader.py`, and `net.py` files. \n",
    "\n",
    "You may want to update file paths or parameter settings where needed (e.g., dataset locations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "We begin by importing libraries, configuring a logger, and enabling inline plotting so that any figures are shown within the notebook. Please ensure that all dependencies (e.g. `torch`, `numpy`, `matplotlib`, `tqdm`) are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#    Imports and Environment Settings #\n",
    "#######################################\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For a notebook, we can display plots inline:\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, RandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure GPU is available if you have one\n",
    "cuda_exist = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_exist else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions\n",
    "The original code uses `utils.py` for parameter loading, checkpoint handling, logging setup, and so forth. Below, we place a subset of those utilities in the notebook for clarity.\n",
    "\n",
    "Please, note that for this notebook the params are selected later while, if you run `main.py`, a file `params.json` contains the default parameters to use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# 2. Utility Functions   #\n",
    "##########################\n",
    "\n",
    "class Params:\n",
    "    \"\"\"\n",
    "    Loads hyperparameters from a json file.\n",
    "    Example structure in params.json:\n",
    "    {\n",
    "        \"num_class\": 370,\n",
    "        \"embedding_dim\": 50,\n",
    "        \"cov_dim\": 2,\n",
    "        \"lstm_hidden_dim\": 50,\n",
    "        \"lstm_layers\": 3,\n",
    "        \"lstm_dropout\": 0.2,\n",
    "        \"train_window\": 24,\n",
    "        \"predict_batch\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"num_epochs\": 5,\n",
    "        \"predict_steps\": 24,\n",
    "        \"predict_start\": 100,\n",
    "        \"sample_times\": 100\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path) as f:\n",
    "            params = json.load(f)\n",
    "            self.__dict__.update(params)\n",
    "        self.device = device  # inject the device (cuda or cpu)\n",
    "        self.model_dir = None\n",
    "        self.plot_dir = None\n",
    "        self.relative_metrics = False\n",
    "        self.sampling = False\n",
    "\n",
    "def set_logger(log_path):\n",
    "    \"\"\"\n",
    "    Sets up the logger to log info in both shell and file `log_path`.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    if not logger.handlers:\n",
    "        # Logging to a file\n",
    "        file_handler = logging.FileHandler(log_path)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "def save_checkpoint(state, checkpoint, epoch, is_best=False):\n",
    "    \"\"\"\n",
    "    Saves model and training parameters at checkpoint + 'last.pth.tar'\n",
    "    If is_best==True, also saves checkpoint + 'best.pth.tar'\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(checkpoint, f'epoch_{epoch+1}.pth.tar')\n",
    "    torch.save(state, filepath)\n",
    "    last_filepath = os.path.join(checkpoint, 'last.pth.tar')\n",
    "    torch.save(state, last_filepath)\n",
    "    if is_best:\n",
    "        best_filepath = os.path.join(checkpoint, 'best.pth.tar')\n",
    "        torch.save(state, best_filepath)\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "    \"\"\"\n",
    "    Loads model parameters (state_dict) from file_path. \n",
    "    If optimizer is provided, loads state_dict of optimizer as well.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise(\"File doesn't exist {}\".format(checkpoint))\n",
    "    checkpoint_data = torch.load(checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint_data['state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint_data['optim_dict'])\n",
    "    return checkpoint_data\n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"\n",
    "    Saves dict of floats in json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        d = {k: float(v) for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)\n",
    "\n",
    "def plot_all_epoch(data, name, location):\n",
    "    \"\"\"\n",
    "    Plots a list or array of values (e.g. ND or loss) over epochs.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(data, label=name)\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(location, f'{name}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "Dataloaders (`TrainDataset` and `TestDataset` classes) and `WeightedSampler` class.\n",
    "\n",
    "If the training is excessively slow, you can choose the number of samples to use when calling the sampler `sampler = WeightedSampler(data_dir, args.dataset, max_samples=5000)` (see below). Please, bear in mind that doing this the model won't converge to a proper solution and results reported in test will not be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "\n",
    "logger = logging.getLogger('DeepAR.Data')\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_path, data_name, num_class):\n",
    "        self.data = np.load(os.path.join(data_path, f'train_data_{data_name}.npy'))\n",
    "        self.label = np.load(os.path.join(data_path, f'train_label_{data_name}.npy'))\n",
    "        self.train_len = self.data.shape[0]\n",
    "        logger.info(f'train_len: {self.train_len}')\n",
    "        logger.info(f'building datasets from {data_path}...')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index,:,:-1],int(self.data[index,0,-1]), self.label[index])\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_path, data_name, num_class):\n",
    "        self.data = np.load(os.path.join(data_path, f'test_data_{data_name}.npy'))\n",
    "        self.v = np.load(os.path.join(data_path, f'test_v_{data_name}.npy'))\n",
    "        self.label = np.load(os.path.join(data_path, f'test_label_{data_name}.npy'))\n",
    "        self.test_len = self.data.shape[0]\n",
    "        logger.info(f'test_len: {self.test_len}')\n",
    "        logger.info(f'building datasets from {data_path}...')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index,:,:-1],int(self.data[index,0,-1]),self.v[index],self.label[index])\n",
    "\n",
    "\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    def __init__(self, data_path, data_name, replacement=True, max_samples=None):\n",
    "        # Load v as before\n",
    "        v = np.load(os.path.join(data_path, f'train_v_{data_name}.npy'))\n",
    "\n",
    "        self.weights = torch.as_tensor(\n",
    "            np.abs(v[:, 0]) / np.sum(np.abs(v[:, 0])),\n",
    "            dtype=torch.double\n",
    "        )\n",
    "        logger.info(f'weights: {self.weights}')\n",
    "\n",
    "        # If max_samples is specified, reduce the total number of samples\n",
    "        if max_samples is not None:\n",
    "            self.num_samples = min(len(self.weights), max_samples)\n",
    "        else:\n",
    "            self.num_samples = len(self.weights)\n",
    "\n",
    "        self.replacement = replacement\n",
    "        logger.info(f'num samples (per epoch): {self.num_samples}')\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Grab a subset of indices based on self.num_samples\n",
    "        # when replacement=True, use multinomial; otherwise a variation:\n",
    "        return iter(\n",
    "            torch.multinomial(self.weights, self.num_samples, self.replacement)\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DeepAR\n",
    "\n",
    "In the following cell we implement the `Net` class (an embedding plus the LSTM), along with the Gaussian log-likelihood loss function and the error metrics.\n",
    "\n",
    "The structure of DeepAR is shown in the following figure (left shows how the model operates during training and right during forecasting/testing).\n",
    "\n",
    "\n",
    "![title](notebook_images/model.png)\n",
    "\n",
    "Usual metrics for forecasting evaluation are the Normalized Deviation (ND) and Root Mean Square Error (RMSE):\n",
    "\n",
    "\\begin{equation} \n",
    "\t\\begin{split}\n",
    "\t\\mathrm{ND} &=\\frac{\\sum_{i, t}\\left|z_{i, t}-\\hat{z}_{i, t}\\right|}{\\sum_{i, t}\\left|z_{i, t}\\right|} \\\\[5pt]\n",
    "\t\\text { RMSE } &=\\frac{\\sqrt{\\frac{1}{N\\left(T-t_{0}\\right)} \\sum_{i, t}\\left(z_{i, t}-\\hat{z}_{i, t}\\right)^{2}}}{\\frac{1}{N\\left(T-t_{0}\\right)} \\sum_{i, t}\\left|z_{i, t}\\right|}\\\\[5pt]\n",
    "\t\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Also, the quantile loss, $QL_{\\rho}$, with $\\rho \\in (0, 1)$ are commonly used:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\begin{split}\n",
    "\t\\mathrm{QL}_{\\rho}(z, \\hat{z})&=2 \\frac{\\sum_{i, t} P_{\\rho}\\left(z_{t}^{(i)}, \\hat{z}_{t}^{(i)}\\right)}{\\sum_{i, t}\\left|z_{t}^{(i)}\\right|},\\\\\n",
    "\t\\quad P_{\\rho}(z, \\hat{z})&=\\left\\{\\begin{array}{ll}\n",
    "\t\\rho(z-\\hat{z}) & \\text { if } z>\\hat{z}, \\\\\n",
    "\t(1-\\rho)(\\hat{z}-z) & \\text { otherwise }\n",
    "\t\\end{array}\\right.\n",
    "\t\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 4. Model Architecture #\n",
    "#########################\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepAR model with:\n",
    "      - an Embedding for categorical features (time series ID)\n",
    "      - LSTM for sequence processing\n",
    "      - output layers to predict mean (mu) and std dev (sigma)\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "        self.params = params\n",
    "        self.embedding = nn.Embedding(params.num_class, params.embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1 + params.cov_dim + params.embedding_dim,\n",
    "            hidden_size=params.lstm_hidden_dim,\n",
    "            num_layers=params.lstm_layers,\n",
    "            bias=True,\n",
    "            batch_first=False,\n",
    "            dropout=params.lstm_dropout\n",
    "        )\n",
    "        # Initialise LSTM forget gate bias\n",
    "        for names in self.lstm._all_weights:\n",
    "            for name in filter(lambda n: \"bias\" in n, names):\n",
    "                bias = getattr(self.lstm, name)\n",
    "                n = bias.size(0)\n",
    "                start, end = n // 4, n // 2\n",
    "                bias.data[start:end].fill_(1.)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.distribution_mu = nn.Linear(params.lstm_hidden_dim * params.lstm_layers, 1)\n",
    "        self.distribution_presigma = nn.Linear(params.lstm_hidden_dim * params.lstm_layers, 1)\n",
    "        self.distribution_sigma = nn.Softplus()\n",
    "\n",
    "    def forward(self, x, idx, hidden, cell):\n",
    "        \"\"\"\n",
    "        x: [1, batch_size, 1+cov_dim] (z_{t-1} + x_t)\n",
    "        idx: [1, batch_size] (time series IDs)\n",
    "        hidden, cell: LSTM hidden state\n",
    "        Returns: mu, sigma, hidden, cell\n",
    "        \"\"\"\n",
    "        # Embedding for the time series ID\n",
    "        onehot_embed = self.embedding(idx)\n",
    "        lstm_input = torch.cat((x, onehot_embed), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "\n",
    "        hidden_permute = hidden.permute(1, 2, 0).contiguous().view(hidden.shape[1], -1)\n",
    "        pre_sigma = self.distribution_presigma(hidden_permute)\n",
    "        mu = self.distribution_mu(hidden_permute)\n",
    "        sigma = self.distribution_sigma(pre_sigma)\n",
    "        return torch.squeeze(mu), torch.squeeze(sigma), hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(\n",
    "            self.params.lstm_layers, batch_size, self.params.lstm_hidden_dim, \n",
    "            device=self.params.device\n",
    "        )\n",
    "\n",
    "    def init_cell(self, batch_size):\n",
    "        return torch.zeros(\n",
    "            self.params.lstm_layers, batch_size, self.params.lstm_hidden_dim, \n",
    "            device=self.params.device\n",
    "        )\n",
    "\n",
    "    def test(self, x, v_batch, id_batch, hidden, cell, sampling=False):\n",
    "        \"\"\"\n",
    "        Inference method for test time, optionally sampling multiple draws.\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[1]\n",
    "        if sampling:\n",
    "            samples = torch.zeros(self.params.sample_times, batch_size, self.params.predict_steps,\n",
    "                                  device=self.params.device)\n",
    "            decoder_hidden = hidden\n",
    "            decoder_cell = cell\n",
    "\n",
    "            for j in range(self.params.sample_times):\n",
    "                decoder_hidden = hidden\n",
    "                decoder_cell = cell\n",
    "                # copy x so each sample's trajectory is independent\n",
    "                x_sample = x.clone()  \n",
    "                for t in range(self.params.predict_steps):\n",
    "                    mu_de, sigma_de, decoder_hidden, decoder_cell = self(\n",
    "                        x_sample[self.params.test_predict_start + t].unsqueeze(0),\n",
    "                        id_batch,\n",
    "                        decoder_hidden,\n",
    "                        decoder_cell\n",
    "                    )\n",
    "                    gaussian = torch.distributions.normal.Normal(mu_de, sigma_de)\n",
    "                    pred = gaussian.sample()\n",
    "                    samples[j, :, t] = pred * v_batch[:, 0] + v_batch[:, 1]\n",
    "                    if t < (self.params.predict_steps - 1):\n",
    "                        x_sample[self.params.test_predict_start + t + 1, :, 0] = pred\n",
    "            sample_mu = torch.median(samples, dim=0)[0]\n",
    "            sample_sigma = samples.std(dim=0)\n",
    "            return samples, sample_mu, sample_sigma\n",
    "        else:\n",
    "            decoder_hidden = hidden\n",
    "            decoder_cell = cell\n",
    "            sample_mu = torch.zeros(batch_size, self.params.predict_steps, device=self.params.device)\n",
    "            sample_sigma = torch.zeros(batch_size, self.params.predict_steps, device=self.params.device)\n",
    "            x_sample = x.clone()\n",
    "            for t in range(self.params.predict_steps):\n",
    "                mu_de, sigma_de, decoder_hidden, decoder_cell = self(\n",
    "                    x_sample[self.params.test_predict_start + t].unsqueeze(0),\n",
    "                    id_batch,\n",
    "                    decoder_hidden,\n",
    "                    decoder_cell\n",
    "                )\n",
    "                sample_mu[:, t] = mu_de * v_batch[:, 0] + v_batch[:, 1]\n",
    "                sample_sigma[:, t] = sigma_de * v_batch[:, 0]\n",
    "                if t < (self.params.predict_steps - 1):\n",
    "                    x_sample[self.params.test_predict_start + t + 1, :, 0] = mu_de\n",
    "            return sample_mu, sample_sigma\n",
    "\n",
    "# The loss function for training\n",
    "def loss_fn(mu, sigma, labels):\n",
    "    zero_index = (labels != 0)\n",
    "    distribution = torch.distributions.normal.Normal(mu[zero_index], sigma[zero_index])\n",
    "    likelihood = distribution.log_prob(labels[zero_index])\n",
    "    return -torch.mean(likelihood)\n",
    "\n",
    "\n",
    "############################\n",
    "#    Evaluation Metrics    #\n",
    "############################\n",
    "\n",
    "def accuracy_ND(mu, labels, relative=False):\n",
    "    \"\"\"\n",
    "    ND = sum(|mu - labels|)/sum(|labels|)\n",
    "    If relative = True, we do a simple average over the number of samples instead.\n",
    "    \"\"\"\n",
    "    zero_index = (labels != 0)\n",
    "    if relative:\n",
    "        diff = torch.mean(torch.abs(mu[zero_index] - labels[zero_index])).item()\n",
    "        return [diff, 1]\n",
    "    else:\n",
    "        diff = torch.sum(torch.abs(mu[zero_index] - labels[zero_index])).item()\n",
    "        summation = torch.sum(torch.abs(labels[zero_index])).item()\n",
    "        return [diff, summation]\n",
    "\n",
    "def accuracy_RMSE(mu, labels, relative=False):\n",
    "    zero_index = (labels != 0)\n",
    "    diff = torch.sum((mu[zero_index] - labels[zero_index])**2).item()\n",
    "    if relative:\n",
    "        return [diff, torch.sum(zero_index).item(), torch.sum(zero_index).item()]\n",
    "    else:\n",
    "        summation = torch.sum(torch.abs(labels[zero_index])).item()\n",
    "        return [diff, summation, torch.sum(zero_index).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can been seen in the previous cell, DeepAR rparametrizes the Gaussian likelihood using its mean and standard deviation, $\\theta = (\\mu, \\sigma)$ where the mean is given by an affine function of the network output, and the standard deviation is obtained by applying an affine transformation followed by a softplus activation in order to ensure $\\sigma > 0$:\n",
    "\n",
    "\n",
    "\n",
    "![title](notebook_images/parametrization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bringing It All Together\n",
    "Now we construct the necessary objects, load datasets, and run the training and evaluation loop. This mirrors the code under the `if __name__ == \"__main__\"` block in `main.py`.\n",
    "\n",
    "For training the model's parameters, we will maximize the log-likelihood:\n",
    "\n",
    "![title](notebook_images/loss.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#    Training and Evaluation Code #\n",
    "###################################\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, test_loader, params, epoch):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch on the training set.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_epoch = np.zeros(len(train_loader))\n",
    "\n",
    "    for i, (train_batch, idx, labels_batch) in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        batch_size = train_batch.shape[0]\n",
    "\n",
    "        train_batch = train_batch.permute(1, 0, 2).to(torch.float32).to(params.device)\n",
    "        labels_batch = labels_batch.permute(1, 0).to(torch.float32).to(params.device)\n",
    "        idx = idx.unsqueeze(0).to(params.device)\n",
    "\n",
    "        loss_val = torch.zeros(1, device=params.device)\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        cell = model.init_cell(batch_size)\n",
    "\n",
    "        for t in range(params.train_window):\n",
    "            # If z_t is missing, replace it with mu from last time step\n",
    "            zero_index = (train_batch[t, :, 0] == 0)\n",
    "            if t > 0 and torch.sum(zero_index) > 0:\n",
    "                train_batch[t, zero_index, 0] = mu[zero_index]\n",
    "            mu, sigma, hidden, cell = model(train_batch[t].unsqueeze(0).clone(), idx, hidden, cell)\n",
    "            loss_val += loss_fn(mu, sigma, labels_batch[t])\n",
    "\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val_item = loss_val.item() / params.train_window\n",
    "        loss_epoch[i] = loss_val_item\n",
    "\n",
    "        # Optional mid-epoch evaluation on the test set:\n",
    "        if i % 1000 == 0:\n",
    "            test_metrics = evaluate(model, loss_fn, test_loader, params, epoch, sample=params.sampling)\n",
    "            model.train()\n",
    "            logging.info(f'Iteration {i} | train_loss: {loss_val_item:.4f}')\n",
    "\n",
    "    return loss_epoch\n",
    "\n",
    "def evaluate(model, loss_fn, test_loader, params, epoch, sample=False):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set. Returns a dictionary of metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        total_nd_num = 0.0\n",
    "        total_nd_den = 0.0\n",
    "        total_rmse_num = 0.0\n",
    "        total_rmse_den = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i, (test_batch, idx, v_batch, labels_batch) in enumerate(test_loader):\n",
    "            batch_size = test_batch.shape[0]\n",
    "\n",
    "            test_batch = test_batch.permute(1, 0, 2).to(torch.float32).to(params.device)\n",
    "            labels_batch = labels_batch.permute(1, 0).to(torch.float32).to(params.device)\n",
    "            idx = idx.unsqueeze(0).to(params.device)\n",
    "            v_batch = v_batch.to(torch.float32).to(params.device)\n",
    "\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            cell = model.init_cell(batch_size)\n",
    "\n",
    "            # Encode the first part of the sequence\n",
    "            for t in range(params.test_predict_start):\n",
    "                zero_index = (test_batch[t, :, 0] == 0)\n",
    "                if t > 0 and torch.sum(zero_index) > 0:\n",
    "                    test_batch[t, zero_index, 0] = mu[zero_index]\n",
    "                mu, sigma, hidden, cell = model(test_batch[t].unsqueeze(0).clone(), idx, hidden, cell)\n",
    "\n",
    "            # Generate predictions\n",
    "            if sample:\n",
    "                samples, sample_mu, sample_sigma = model.test(test_batch, v_batch, idx, hidden, cell, sampling=True)\n",
    "                # For metrics, we can use sample_mu as our point forecast\n",
    "                mu_out = sample_mu\n",
    "            else:\n",
    "                sample_mu, sample_sigma = model.test(test_batch, v_batch, idx, hidden, cell, sampling=False)\n",
    "                mu_out = sample_mu\n",
    "\n",
    "            # We only evaluate on the predicted part\n",
    "            labels_eval = labels_batch[params.test_predict_start: params.test_predict_start + params.predict_steps, :]\n",
    "            mu_eval = mu_out.permute(1, 0)\n",
    "\n",
    "            # Expand or contract if needed\n",
    "            if mu_eval.shape[0] != labels_eval.shape[0]:\n",
    "                min_len = min(mu_eval.shape[0], labels_eval.shape[0])\n",
    "                mu_eval = mu_eval[:min_len, :]\n",
    "                labels_eval = labels_eval[:min_len, :]\n",
    "\n",
    "            # Flatten or keep as is for ND and RMSE\n",
    "            nd_val = accuracy_ND(mu_eval, labels_eval, relative=params.relative_metrics)\n",
    "            rmse_val = accuracy_RMSE(mu_eval, labels_eval, relative=params.relative_metrics)\n",
    "\n",
    "            total_nd_num += nd_val[0]\n",
    "            total_nd_den += nd_val[1]\n",
    "            total_rmse_num += rmse_val[0]\n",
    "            total_rmse_den += rmse_val[1]\n",
    "            total_count += rmse_val[2]\n",
    "\n",
    "        ND = total_nd_num / total_nd_den if total_nd_den != 0 else 0\n",
    "        RMSE = np.sqrt(total_rmse_num / total_count) if total_count != 0 else 0\n",
    "\n",
    "    metrics = {'ND': ND, 'RMSE': RMSE}\n",
    "    logging.info(f\"Epoch {epoch+1} Evaluation - ND: {ND:.4f}, RMSE: {RMSE:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, loss_fn, params, restore_file=None):\n",
    "    \"\"\"\n",
    "    Full training loop, includes checkpointing, best-model tracking, and plotting.\n",
    "    \"\"\"\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(params.model_dir, restore_file + '.pth.tar')\n",
    "        logging.info(f'Restoring parameters from {restore_path}')\n",
    "        load_checkpoint(restore_path, model, optimizer)\n",
    "\n",
    "    logging.info('Begin training and evaluation')\n",
    "    best_test_ND = float('inf')\n",
    "    train_len = len(train_loader)\n",
    "    ND_summary = np.zeros(params.num_epochs)\n",
    "    loss_summary = np.zeros((train_len * params.num_epochs))\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        logging.info(f'Epoch {epoch+1}/{params.num_epochs}')\n",
    "        epoch_loss = train(model, optimizer, loss_fn, train_loader, test_loader, params, epoch)\n",
    "        loss_summary[epoch * train_len : (epoch + 1) * train_len] = epoch_loss\n",
    "\n",
    "        test_metrics = evaluate(model, loss_fn, test_loader, params, epoch, sample=params.sampling)\n",
    "        ND_summary[epoch] = test_metrics['ND']\n",
    "        is_best = (ND_summary[epoch] <= best_test_ND)\n",
    "\n",
    "        # Save model\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optim_dict': optimizer.state_dict()\n",
    "        }, checkpoint=params.model_dir, epoch=epoch, is_best=is_best)\n",
    "\n",
    "        if is_best:\n",
    "            logging.info('- Found new best ND')\n",
    "            best_test_ND = ND_summary[epoch]\n",
    "            best_json_path = os.path.join(params.model_dir, 'metrics_test_best_weights.json')\n",
    "            save_dict_to_json(test_metrics, best_json_path)\n",
    "\n",
    "        logging.info('Current Best ND is: {:.5f}'.format(best_test_ND))\n",
    "\n",
    "        # Plots\n",
    "        plot_all_epoch(ND_summary[:epoch + 1], f\"{args_dataset}_ND\", params.plot_dir)\n",
    "        plot_all_epoch(loss_summary[: (epoch + 1)*train_len], f\"{args_dataset}_loss\", params.plot_dir)\n",
    "\n",
    "        last_json_path = os.path.join(params.model_dir, 'metrics_test_last_weights.json')\n",
    "        save_dict_to_json(test_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the code, we define here the value of DeepAR's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    \"\"\"Simple class for holding hyperparameters.\"\"\"\n",
    "    def __init__(self):\n",
    "        # define them directly in code\n",
    "        self.learning_rate = 1e-3\n",
    "        self.batch_size = 64\n",
    "        self.lstm_layers = 3\n",
    "        self.num_epochs = 10\n",
    "        self.train_window = 192     #24 datapoints per day * 7 days\n",
    "        self.test_window = 192\n",
    "        self.test_predict_start = 168 \n",
    "        self.predict_steps = 24\n",
    "        self.num_class = 370\n",
    "        self.cov_dim = 4\n",
    "        self.lstm_hidden_dim = 40\n",
    "        self.embedding_dim = 20\n",
    "        self.sample_times = 200\n",
    "        self.lstm_dropout = 0.1\n",
    "        self.predict_batch = 256\n",
    "\n",
    "        # device: CPU or GPU\n",
    "        import torch\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the model\n",
    "\n",
    "We will be training the model on the electricity dataset (see Readme for downloading instructions). It consists os 370 time series with hourly granularity. We will be using time series of length 192 (7 days). During training, we will use 168 steps as conditioning range and 24 as forecasting range.\n",
    "\n",
    "\n",
    "Please, note that the code (publicly available at https://github.com/husnejahan/DeepAR-pytorch/tree/master) does not use a validation dataset. This implies that they are validating the performance on the test set (which is a very bad practice). The easiest fix would be to obtain a val_set from train_set and a val_loader from it, using that data partition when evaluating the model inside the training loop and using test_loader exclusively at the end of the training to evaluate the final model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "train_len: 389101\n",
      "building datasets from data/elect...\n",
      "test_len: 2590\n",
      "building datasets from data/elect...\n",
      "weights: tensor([4.5579e-08, 4.7663e-08, 4.8446e-08,  ..., 8.4827e-05, 8.4776e-05,\n",
      "        8.4272e-05], dtype=torch.float64)\n",
      "num samples (per epoch): 389101\n",
      "Loading complete.\n",
      "Model:\n",
      "Net(\n",
      "  (embedding): Embedding(370, 20)\n",
      "  (lstm): LSTM(25, 40, num_layers=3, dropout=0.1)\n",
      "  (relu): ReLU()\n",
      "  (distribution_mu): Linear(in_features=120, out_features=1, bias=True)\n",
      "  (distribution_presigma): Linear(in_features=120, out_features=1, bias=True)\n",
      "  (distribution_sigma): Softplus(beta=1.0, threshold=20.0)\n",
      ")\n",
      "Starting training for 10 epoch(s)...\n",
      "Begin training and evaluation\n",
      "Epoch 1/10\n",
      "  0%|          | 0/6080 [00:00<?, ?it/s]Epoch 1 Evaluation - ND: 0.9090, RMSE: 21063.9253\n",
      "Iteration 0 | train_loss: 1.7676\n",
      "  1%|          | 33/6080 [00:08<21:12,  4.75it/s] Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1103f8c70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fernandomp/anaconda3/envs/dlts/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "  8%|â–Š         | 516/6080 [01:51<20:02,  4.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 7) Train\u001b[39;00m\n\u001b[1;32m     49\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epoch(s)...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeepAR_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestore_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 130\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, test_loader, optimizer, loss_fn, params, restore_file)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m    129\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     loss_summary[epoch \u001b[38;5;241m*\u001b[39m train_len : (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m train_len] \u001b[38;5;241m=\u001b[39m epoch_loss\n\u001b[1;32m    133\u001b[0m     test_metrics \u001b[38;5;241m=\u001b[39m evaluate(model, loss_fn, test_loader, params, epoch, sample\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39msampling)\n",
      "Cell \u001b[0;32mIn[48], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, train_loader, test_loader, params, epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m     mu, sigma, hidden, cell \u001b[38;5;241m=\u001b[39m model(train_batch[t]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mclone(), idx, hidden, cell)\n\u001b[1;32m     30\u001b[0m     loss_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(mu, sigma, labels_batch[t])\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m loss_val_item \u001b[38;5;241m=\u001b[39m loss_val\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m params\u001b[38;5;241m.\u001b[39mtrain_window\n",
      "File \u001b[0;32m~/anaconda3/envs/dlts/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dlts/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dlts/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Replace these with your own paths/names if required\n",
    "args_dataset = \"elect\"\n",
    "data_folder = \"data\"\n",
    "model_name = \"experiments/base_model\"\n",
    "restore_file = None  # e.g. \"epoch_5\" if you want to resume\n",
    "relative_metrics = False\n",
    "sampling = False\n",
    "\n",
    "# 1) Load parameters from JSON\n",
    "model_dir = os.path.join(model_name)\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "params = Params()\n",
    "\n",
    "params.relative_metrics = relative_metrics\n",
    "params.sampling = sampling\n",
    "params.model_dir = model_dir\n",
    "params.plot_dir = os.path.join(model_dir, 'figures')\n",
    "\n",
    "# Create plot_dir if it doesn't exist\n",
    "os.makedirs(params.plot_dir, exist_ok=True)\n",
    "\n",
    "# 2) Prepare logging\n",
    "set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "# 3) Create Datasets and Loaders\n",
    "logging.info('Loading datasets...')\n",
    "data_dir = os.path.join(data_folder, args_dataset)\n",
    "\n",
    "train_set = TrainDataset(data_dir, args_dataset, params.num_class)\n",
    "test_set = TestDataset(data_dir, args_dataset, params.num_class)\n",
    "sampler = WeightedSampler(data_dir, args_dataset)\n",
    "#sampler = WeightedSampler(data_dir, args_dataset, max_samples=5000)\n",
    "train_loader = DataLoader(train_set, batch_size=params.batch_size, sampler=sampler, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=params.predict_batch, sampler=RandomSampler(test_set), num_workers=0)\n",
    "logging.info('Loading complete.')\n",
    "\n",
    "# 4) Instantiate Model\n",
    "model = Net(params).to(params.device)\n",
    "\n",
    "logging.info(f'Model:\\n{model}')\n",
    "\n",
    "# 5) Optimiser\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "# 6) Loss Function\n",
    "deepAR_loss = loss_fn\n",
    "\n",
    "# 7) Train\n",
    "logging.info(f'Starting training for {params.num_epochs} epoch(s)...')\n",
    "train_and_evaluate(model, train_loader, test_loader, optimizer, deepAR_loss, params, restore_file=restore_file)\n",
    "logging.info('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of training (when finishing the first epoch), the predictions of the model will look something like this:\n",
    "\n",
    "![title](notebook_images/0.png)\n",
    "\n",
    "At epoch 11:\n",
    "\n",
    "![title](notebook_images/11.png)\n",
    "\n",
    "With loss function (all iterations):\n",
    "\n",
    "![title](notebook_images/elect_loss_summary.png)\n",
    "\n",
    "And ND error (per epoch):\n",
    "\n",
    "![title](notebook_images/elect_ND_summary.png)\n",
    "\n",
    "For performance comparable to the paper's reported value, you should train for at least 20 epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
