{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IE CDT Time Series Analysis and Signal Processing Lab\n",
    "\n",
    "## Deep Learning for Time Series Forecasting\n",
    "\n",
    "#### Fernando Moreno-Pino, University of Oxford (fernando.moreno-pino@eng.ox.ac.uk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR Time Series Forecasting (Training and Testing)\n",
    "\n",
    "This notebook shows how to train and test your DeepAR time series forecasting model. For a full understanding on how the model is implemented, check the following files: `main.py`, `evaluate.py`, `dataloader.py`, and `net.py`.  You may want to update file paths or parameter settings where needed (e.g., dataset locations).\n",
    "\n",
    "- You should first check the repository’s README file for information on how to download the datasets and install the Conda environment.\n",
    "\n",
    "- Disclaimer: the DeepAR implementation used in this tutorial is publicly available here: https://github.com/husnejahan/DeepAR-pytorch/tree/master.\n",
    "\n",
    "-Note: images in this notebook may not be propertly shown in Github, so please download it and run it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup & Imports\n",
    "\n",
    "\n",
    "We begin by importing libraries and configuring a logger. Please ensure that all dependencies (e.g. `torch`, `numpy`, `matplotlib`, `tqdm`, etc) are installed. See `requirements.txt` for the full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#    Imports and Environment Settings #\n",
    "#######################################\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For a notebook, we can display plots inline:\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, RandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure GPU is available if you have one\n",
    "cuda_exist = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda_exist else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Utility Functions\n",
    "The original code uses `utils.py` for parameter loading, checkpoint handling, logging setup, and so forth. Below, we place a subset of those utilities in the notebook for clarity.\n",
    "\n",
    "Please, note that for this notebook the params are explicitly selected in a cell below, while if you run `main.py`, a file `params.json` contains the default parameters to use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "#    Utility Functions   #\n",
    "##########################\n",
    "\n",
    "class Params:\n",
    "    \"\"\"\n",
    "    Loads hyperparameters from a json file if running main.py. In this notebook, the params are selected in a cell below.\n",
    "    Example structure in params.json:\n",
    "    {\n",
    "        \"num_class\": 370,\n",
    "        \"embedding_dim\": 50,\n",
    "        \"cov_dim\": 2,\n",
    "        \"lstm_hidden_dim\": 50,\n",
    "        \"lstm_layers\": 3,\n",
    "        \"lstm_dropout\": 0.2,\n",
    "        \"train_window\": 24,\n",
    "        \"predict_batch\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"num_epochs\": 5,\n",
    "        \"predict_steps\": 24,\n",
    "        \"predict_start\": 100,\n",
    "        \"sample_times\": 100\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path) as f:\n",
    "            params = json.load(f)\n",
    "            self.__dict__.update(params)\n",
    "        self.device = device  # inject the device (cuda or cpu)\n",
    "        self.model_dir = None\n",
    "        self.plot_dir = None\n",
    "        self.relative_metrics = False\n",
    "        self.sampling = False\n",
    "\n",
    "def set_logger(log_path):\n",
    "    \"\"\"\n",
    "    Sets up the logger to log info in both shell and file `log_path`.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    if not logger.handlers:\n",
    "        # Logging to a file\n",
    "        file_handler = logging.FileHandler(log_path)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "def save_checkpoint(state, checkpoint, epoch, is_best=False):\n",
    "    \"\"\"\n",
    "    Saves model and training parameters at checkpoint + 'last.pth.tar'\n",
    "    If is_best==True, also saves checkpoint + 'best.pth.tar'\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(checkpoint, f'epoch_{epoch+1}.pth.tar')\n",
    "    torch.save(state, filepath)\n",
    "    last_filepath = os.path.join(checkpoint, 'last.pth.tar')\n",
    "    torch.save(state, last_filepath)\n",
    "    if is_best:\n",
    "        best_filepath = os.path.join(checkpoint, 'best.pth.tar')\n",
    "        torch.save(state, best_filepath)\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer=None):\n",
    "    \"\"\"\n",
    "    Loads model parameters (state_dict) from file_path. \n",
    "    If optimizer is provided, loads state_dict of optimizer as well.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint):\n",
    "        raise(\"File doesn't exist {}\".format(checkpoint))\n",
    "    checkpoint_data = torch.load(checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint_data['state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint_data['optim_dict'])\n",
    "    return checkpoint_data\n",
    "\n",
    "def save_dict_to_json(d, json_path):\n",
    "    \"\"\"\n",
    "    Saves dict of floats in json file\n",
    "    \"\"\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        d = {k: float(v) for k, v in d.items()}\n",
    "        json.dump(d, f, indent=4)\n",
    "\n",
    "def plot_all_epoch(data, name, location):\n",
    "    \"\"\"\n",
    "    Plots a list or array of values (e.g. ND or loss) over epochs.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(data, label=name)\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(location, f'{name}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Loading\n",
    "We implement here the dataloaders (`TrainDataset` and `TestDataset` classes) and `WeightedSampler` class. See Section 7 for information regarding the dataset we will be using.\n",
    "\n",
    "If the training is excessively slow, you can choose the number of samples to use when calling the sampler `sampler = WeightedSampler(data_dir, args.dataset, max_samples=5000)` (see below). Please, bear in mind that doing this the model won't converge to a proper solution and results reported in test will not be good. Also, if `len(train_loader) < 1000`, plots of the prediction won't be shown (you can easily modify the code to change it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "\n",
    "logger = logging.getLogger('DeepAR.Data')\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_path, data_name, num_class):\n",
    "        self.data = np.load(os.path.join(data_path, f'train_data_{data_name}.npy'))\n",
    "        self.label = np.load(os.path.join(data_path, f'train_label_{data_name}.npy'))\n",
    "        self.train_len = self.data.shape[0]\n",
    "        logger.info(f'train_len: {self.train_len}')\n",
    "        logger.info(f'building datasets from {data_path}...')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index,:,:-1],int(self.data[index,0,-1]), self.label[index])\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_path, data_name, num_class):\n",
    "        self.data = np.load(os.path.join(data_path, f'test_data_{data_name}.npy'))\n",
    "        self.v = np.load(os.path.join(data_path, f'test_v_{data_name}.npy'))\n",
    "        self.label = np.load(os.path.join(data_path, f'test_label_{data_name}.npy'))\n",
    "        self.test_len = self.data.shape[0]\n",
    "        logger.info(f'test_len: {self.test_len}')\n",
    "        logger.info(f'building datasets from {data_path}...')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index,:,:-1],int(self.data[index,0,-1]),self.v[index],self.label[index])\n",
    "\n",
    "\n",
    "\n",
    "class WeightedSampler(Sampler):\n",
    "    def __init__(self, data_path, data_name, replacement=True, max_samples=None):\n",
    "        # Load v as before\n",
    "        v = np.load(os.path.join(data_path, f'train_v_{data_name}.npy'))\n",
    "\n",
    "        self.weights = torch.as_tensor(\n",
    "            np.abs(v[:, 0]) / np.sum(np.abs(v[:, 0])),\n",
    "            dtype=torch.double\n",
    "        )\n",
    "        logger.info(f'weights: {self.weights}')\n",
    "\n",
    "        # If max_samples is specified, reduce the total number of samples\n",
    "        if max_samples is not None:\n",
    "            self.num_samples = min(len(self.weights), max_samples)\n",
    "        else:\n",
    "            self.num_samples = len(self.weights)\n",
    "\n",
    "        self.replacement = replacement\n",
    "        logger.info(f'num samples (per epoch): {self.num_samples}')\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Grab a subset of indices based on self.num_samples\n",
    "        # when replacement=True, use multinomial; otherwise a variation:\n",
    "        return iter(\n",
    "            torch.multinomial(self.weights, self.num_samples, self.replacement)\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. DeepAR\n",
    "\n",
    "In the following cell we implement the `Net` class (an embedding plus the LSTM), along with the Gaussian log-likelihood loss function and the error metrics.\n",
    "\n",
    "The structure of DeepAR is shown in the following figure (**Left** shows how the model operates during training. **Right** shows how the model operates during forecasting/testing).\n",
    "\n",
    "\n",
    "![title](notebook_images/model.png)\n",
    "\n",
    "Usual metrics for forecasting evaluation are the Normalized Deviation (ND) and Root Mean Square Error (RMSE):\n",
    "\n",
    "\\begin{equation} \n",
    "\t\\begin{split}\n",
    "\t\\mathrm{ND} &=\\frac{\\sum_{i, t}\\left|z_{i, t}-\\hat{z}_{i, t}\\right|}{\\sum_{i, t}\\left|z_{i, t}\\right|} \\\\[5pt]\n",
    "\t\\text { RMSE } &=\\frac{\\sqrt{\\frac{1}{N\\left(T-t_{0}\\right)} \\sum_{i, t}\\left(z_{i, t}-\\hat{z}_{i, t}\\right)^{2}}}{\\frac{1}{N\\left(T-t_{0}\\right)} \\sum_{i, t}\\left|z_{i, t}\\right|}\\\\[5pt]\n",
    "\t\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Also, the quantile loss, $QL_{\\rho}$, with $\\rho \\in (0, 1)$ are commonly used:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\begin{split}\n",
    "\t\\mathrm{QL}_{\\rho}(z, \\hat{z})&=2 \\frac{\\sum_{i, t} P_{\\rho}\\left(z_{t}^{(i)}, \\hat{z}_{t}^{(i)}\\right)}{\\sum_{i, t}\\left|z_{t}^{(i)}\\right|},\\\\\n",
    "\t\\quad P_{\\rho}(z, \\hat{z})&=\\left\\{\\begin{array}{ll}\n",
    "\t\\rho(z-\\hat{z}) & \\text { if } z>\\hat{z}, \\\\\n",
    "\t(1-\\rho)(\\hat{z}-z) & \\text { otherwise }\n",
    "\t\\end{array}\\right.\n",
    "\t\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#    Model Architecture #\n",
    "#########################\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepAR model with:\n",
    "      - an Embedding for categorical features (time series ID)\n",
    "      - LSTM for sequence processing\n",
    "      - output layers to predict mean (mu) and std dev (sigma)\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "        self.params = params\n",
    "        self.embedding = nn.Embedding(params.num_class, params.embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1 + params.cov_dim + params.embedding_dim,\n",
    "            hidden_size=params.lstm_hidden_dim,\n",
    "            num_layers=params.lstm_layers,\n",
    "            bias=True,\n",
    "            batch_first=False,\n",
    "            dropout=params.lstm_dropout\n",
    "        )\n",
    "        # Initialise LSTM forget gate bias\n",
    "        for names in self.lstm._all_weights:\n",
    "            for name in filter(lambda n: \"bias\" in n, names):\n",
    "                bias = getattr(self.lstm, name)\n",
    "                n = bias.size(0)\n",
    "                start, end = n // 4, n // 2\n",
    "                bias.data[start:end].fill_(1.)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.distribution_mu = nn.Linear(params.lstm_hidden_dim * params.lstm_layers, 1)\n",
    "        self.distribution_presigma = nn.Linear(params.lstm_hidden_dim * params.lstm_layers, 1)\n",
    "        self.distribution_sigma = nn.Softplus()\n",
    "\n",
    "    def forward(self, x, idx, hidden, cell):\n",
    "        \"\"\"\n",
    "        x: [1, batch_size, 1+cov_dim] (z_{t-1} + x_t)\n",
    "        idx: [1, batch_size] (time series IDs)\n",
    "        hidden, cell: LSTM hidden state\n",
    "        Returns: mu, sigma, hidden, cell\n",
    "        \"\"\"\n",
    "        # Embedding for the time series ID\n",
    "        onehot_embed = self.embedding(idx)\n",
    "        lstm_input = torch.cat((x, onehot_embed), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "\n",
    "        hidden_permute = hidden.permute(1, 2, 0).contiguous().view(hidden.shape[1], -1)\n",
    "        pre_sigma = self.distribution_presigma(hidden_permute)\n",
    "        mu = self.distribution_mu(hidden_permute)\n",
    "        sigma = self.distribution_sigma(pre_sigma)\n",
    "        return torch.squeeze(mu), torch.squeeze(sigma), hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(\n",
    "            self.params.lstm_layers, batch_size, self.params.lstm_hidden_dim, \n",
    "            device=self.params.device\n",
    "        )\n",
    "\n",
    "    def init_cell(self, batch_size):\n",
    "        return torch.zeros(\n",
    "            self.params.lstm_layers, batch_size, self.params.lstm_hidden_dim, \n",
    "            device=self.params.device\n",
    "        )\n",
    "\n",
    "    def test(self, x, v_batch, id_batch, hidden, cell, sampling=False):\n",
    "        \"\"\"\n",
    "        Inference method for test time, optionally sampling multiple draws.\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[1]\n",
    "        if sampling:\n",
    "            samples = torch.zeros(self.params.sample_times, batch_size, self.params.predict_steps,\n",
    "                                  device=self.params.device)\n",
    "            decoder_hidden = hidden\n",
    "            decoder_cell = cell\n",
    "\n",
    "            for j in range(self.params.sample_times):\n",
    "                decoder_hidden = hidden\n",
    "                decoder_cell = cell\n",
    "                # copy x so each sample's trajectory is independent\n",
    "                x_sample = x.clone()  \n",
    "                for t in range(self.params.predict_steps):\n",
    "                    mu_de, sigma_de, decoder_hidden, decoder_cell = self(\n",
    "                        x_sample[self.params.test_predict_start + t].unsqueeze(0),\n",
    "                        id_batch,\n",
    "                        decoder_hidden,\n",
    "                        decoder_cell\n",
    "                    )\n",
    "                    gaussian = torch.distributions.normal.Normal(mu_de, sigma_de)\n",
    "                    pred = gaussian.sample()\n",
    "                    samples[j, :, t] = pred * v_batch[:, 0] + v_batch[:, 1]\n",
    "                    if t < (self.params.predict_steps - 1):\n",
    "                        x_sample[self.params.test_predict_start + t + 1, :, 0] = pred\n",
    "            sample_mu = torch.median(samples, dim=0)[0]\n",
    "            sample_sigma = samples.std(dim=0)\n",
    "            return samples, sample_mu, sample_sigma\n",
    "        else:\n",
    "            decoder_hidden = hidden\n",
    "            decoder_cell = cell\n",
    "            sample_mu = torch.zeros(batch_size, self.params.predict_steps, device=self.params.device)\n",
    "            sample_sigma = torch.zeros(batch_size, self.params.predict_steps, device=self.params.device)\n",
    "            x_sample = x.clone()\n",
    "            for t in range(self.params.predict_steps):\n",
    "                mu_de, sigma_de, decoder_hidden, decoder_cell = self(\n",
    "                    x_sample[self.params.test_predict_start + t].unsqueeze(0),\n",
    "                    id_batch,\n",
    "                    decoder_hidden,\n",
    "                    decoder_cell\n",
    "                )\n",
    "                sample_mu[:, t] = mu_de * v_batch[:, 0] + v_batch[:, 1]\n",
    "                sample_sigma[:, t] = sigma_de * v_batch[:, 0]\n",
    "                if t < (self.params.predict_steps - 1):\n",
    "                    x_sample[self.params.test_predict_start + t + 1, :, 0] = mu_de\n",
    "            return sample_mu, sample_sigma\n",
    "\n",
    "# The loss function for training\n",
    "def loss_fn(mu, sigma, labels):\n",
    "    zero_index = (labels != 0)\n",
    "    distribution = torch.distributions.normal.Normal(mu[zero_index], sigma[zero_index])\n",
    "    likelihood = distribution.log_prob(labels[zero_index])\n",
    "    return -torch.mean(likelihood)\n",
    "\n",
    "\n",
    "############################\n",
    "#    Evaluation Metrics    #\n",
    "############################\n",
    "\n",
    "def accuracy_ND(mu, labels, relative=False):\n",
    "    \"\"\"\n",
    "    ND = sum(|mu - labels|)/sum(|labels|)\n",
    "    If relative = True, we do a simple average over the number of samples instead.\n",
    "    \"\"\"\n",
    "    zero_index = (labels != 0)\n",
    "    if relative:\n",
    "        diff = torch.mean(torch.abs(mu[zero_index] - labels[zero_index])).item()\n",
    "        return [diff, 1]\n",
    "    else:\n",
    "        diff = torch.sum(torch.abs(mu[zero_index] - labels[zero_index])).item()\n",
    "        summation = torch.sum(torch.abs(labels[zero_index])).item()\n",
    "        return [diff, summation]\n",
    "\n",
    "def accuracy_RMSE(mu, labels, relative=False):\n",
    "    zero_index = (labels != 0)\n",
    "    diff = torch.sum((mu[zero_index] - labels[zero_index])**2).item()\n",
    "    if relative:\n",
    "        return [diff, torch.sum(zero_index).item(), torch.sum(zero_index).item()]\n",
    "    else:\n",
    "        summation = torch.sum(torch.abs(labels[zero_index])).item()\n",
    "        return [diff, summation, torch.sum(zero_index).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can been seen in the previous cell, DeepAR parametrizes the Gaussian likelihood using its mean and standard deviation, $\\theta = (\\mu, \\sigma)$, where the mean is given by a function of the network output, and the standard deviation is obtained by applying a softplus activation in order to ensure $\\sigma > 0$:\n",
    "\n",
    "\n",
    "\n",
    "![title](notebook_images/parametrization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Bringing It All Together\n",
    "Now we construct the necessary objects, load datasets, and run the training and evaluation loop. This mirrors the code under the `if __name__ == \"__main__\"` block in `main.py`.\n",
    "\n",
    "For training the model's parameters, we will maximize the log-likelihood:\n",
    "\n",
    "![title](notebook_images/loss.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#    Training Code                #\n",
    "###################################\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, test_loader, params, epoch):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch on the training set.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_epoch = np.zeros(len(train_loader))\n",
    "\n",
    "    for i, (train_batch, idx, labels_batch) in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        batch_size = train_batch.shape[0]\n",
    "\n",
    "        train_batch = train_batch.permute(1, 0, 2).to(torch.float32).to(params.device)\n",
    "        labels_batch = labels_batch.permute(1, 0).to(torch.float32).to(params.device)\n",
    "        idx = idx.unsqueeze(0).to(params.device)\n",
    "\n",
    "        loss_val = torch.zeros(1, device=params.device)\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        cell = model.init_cell(batch_size)\n",
    "\n",
    "        for t in range(params.train_window):\n",
    "            # If z_t is missing, replace it with mu from last time step\n",
    "            zero_index = (train_batch[t, :, 0] == 0)\n",
    "            if t > 0 and torch.sum(zero_index) > 0:\n",
    "                train_batch[t, zero_index, 0] = mu[zero_index]\n",
    "            mu, sigma, hidden, cell = model(train_batch[t].unsqueeze(0).clone(), idx, hidden, cell)\n",
    "            loss_val += loss_fn(mu, sigma, labels_batch[t])\n",
    "\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val_item = loss_val.item() / params.train_window\n",
    "        loss_epoch[i] = loss_val_item\n",
    "\n",
    "        # Optional mid-epoch evaluation on the test set:\n",
    "        if i % 1000 == 0:\n",
    "            test_metrics = evaluate(model, loss_fn, test_loader, params, epoch, sample=params.sampling)\n",
    "            model.train()\n",
    "            logging.info(f'Iteration {i} | train_loss: {loss_val_item:.4f}')\n",
    "\n",
    "    return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#    Evaluation Code              #\n",
    "###################################\n",
    "\n",
    "def evaluate(model, loss_fn, test_loader, params, epoch, sample=False):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set. Returns a dictionary of metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        total_nd_num = 0.0\n",
    "        total_nd_den = 0.0\n",
    "        total_rmse_num = 0.0\n",
    "        total_rmse_den = 0.0\n",
    "        total_count = 0\n",
    "\n",
    "        for i, (test_batch, idx, v_batch, labels_batch) in enumerate(test_loader):\n",
    "            batch_size = test_batch.shape[0]\n",
    "\n",
    "            test_batch = test_batch.permute(1, 0, 2).to(torch.float32).to(params.device)\n",
    "            labels_batch = labels_batch.permute(1, 0).to(torch.float32).to(params.device)\n",
    "            idx = idx.unsqueeze(0).to(params.device)\n",
    "            v_batch = v_batch.to(torch.float32).to(params.device)\n",
    "\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            cell = model.init_cell(batch_size)\n",
    "\n",
    "            # Encode the first part of the sequence\n",
    "            for t in range(params.test_predict_start):\n",
    "                zero_index = (test_batch[t, :, 0] == 0)\n",
    "                if t > 0 and torch.sum(zero_index) > 0:\n",
    "                    test_batch[t, zero_index, 0] = mu[zero_index]\n",
    "                mu, sigma, hidden, cell = model(test_batch[t].unsqueeze(0).clone(), idx, hidden, cell)\n",
    "\n",
    "            # Generate predictions\n",
    "            if sample:\n",
    "                samples, sample_mu, sample_sigma = model.test(test_batch, v_batch, idx, hidden, cell, sampling=True)\n",
    "                # For metrics, we can use sample_mu as our point forecast\n",
    "                mu_out = sample_mu\n",
    "            else:\n",
    "                sample_mu, sample_sigma = model.test(test_batch, v_batch, idx, hidden, cell, sampling=False)\n",
    "                mu_out = sample_mu\n",
    "\n",
    "            # We only evaluate on the predicted part\n",
    "            labels_eval = labels_batch[params.test_predict_start: params.test_predict_start + params.predict_steps, :]\n",
    "            mu_eval = mu_out.permute(1, 0)\n",
    "\n",
    "            # Expand or contract if needed\n",
    "            if mu_eval.shape[0] != labels_eval.shape[0]:\n",
    "                min_len = min(mu_eval.shape[0], labels_eval.shape[0])\n",
    "                mu_eval = mu_eval[:min_len, :]\n",
    "                labels_eval = labels_eval[:min_len, :]\n",
    "\n",
    "            # Flatten or keep as is for ND and RMSE\n",
    "            nd_val = accuracy_ND(mu_eval, labels_eval, relative=params.relative_metrics)\n",
    "            rmse_val = accuracy_RMSE(mu_eval, labels_eval, relative=params.relative_metrics)\n",
    "\n",
    "            total_nd_num += nd_val[0]\n",
    "            total_nd_den += nd_val[1]\n",
    "            total_rmse_num += rmse_val[0]\n",
    "            total_rmse_den += rmse_val[1]\n",
    "            total_count += rmse_val[2]\n",
    "\n",
    "        ND = total_nd_num / total_nd_den if total_nd_den != 0 else 0\n",
    "        RMSE = np.sqrt(total_rmse_num / total_count) if total_count != 0 else 0\n",
    "\n",
    "    metrics = {'ND': ND, 'RMSE': RMSE}\n",
    "    logging.info(f\"Epoch {epoch+1} Evaluation - ND: {ND:.4f}, RMSE: {RMSE:.4f}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, loss_fn, params, restore_file=None):\n",
    "    \"\"\"\n",
    "    Full training loop, includes checkpointing, best-model tracking, and plotting.\n",
    "    \"\"\"\n",
    "    if restore_file is not None:\n",
    "        restore_path = os.path.join(params.model_dir, restore_file + '.pth.tar')\n",
    "        logging.info(f'Restoring parameters from {restore_path}')\n",
    "        load_checkpoint(restore_path, model, optimizer)\n",
    "\n",
    "    logging.info('Begin training and evaluation')\n",
    "    best_test_ND = float('inf')\n",
    "    train_len = len(train_loader)\n",
    "    ND_summary = np.zeros(params.num_epochs)\n",
    "    loss_summary = np.zeros((train_len * params.num_epochs))\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        logging.info(f'Epoch {epoch+1}/{params.num_epochs}')\n",
    "        epoch_loss = train(model, optimizer, loss_fn, train_loader, test_loader, params, epoch)\n",
    "        loss_summary[epoch * train_len : (epoch + 1) * train_len] = epoch_loss\n",
    "\n",
    "        test_metrics = evaluate(model, loss_fn, test_loader, params, epoch, sample=params.sampling)\n",
    "        ND_summary[epoch] = test_metrics['ND']\n",
    "        is_best = (ND_summary[epoch] <= best_test_ND)\n",
    "\n",
    "        # Save model\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optim_dict': optimizer.state_dict()\n",
    "        }, checkpoint=params.model_dir, epoch=epoch, is_best=is_best)\n",
    "\n",
    "        if is_best:\n",
    "            logging.info('- Found new best ND')\n",
    "            best_test_ND = ND_summary[epoch]\n",
    "            best_json_path = os.path.join(params.model_dir, 'metrics_test_best_weights.json')\n",
    "            save_dict_to_json(test_metrics, best_json_path)\n",
    "\n",
    "        logging.info('Current Best ND is: {:.5f}'.format(best_test_ND))\n",
    "\n",
    "        # Plots\n",
    "        plot_all_epoch(ND_summary[:epoch + 1], f\"{args_dataset}_ND\", params.plot_dir)\n",
    "        plot_all_epoch(loss_summary[: (epoch + 1)*train_len], f\"{args_dataset}_loss\", params.plot_dir)\n",
    "\n",
    "        last_json_path = os.path.join(params.model_dir, 'metrics_test_last_weights.json')\n",
    "        save_dict_to_json(test_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the code, we define here the value of **DeepAR's parameters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    \"\"\"Simple class for holding hyperparameters.\"\"\"\n",
    "    def __init__(self):\n",
    "        # define them directly in code\n",
    "        self.learning_rate = 1e-3\n",
    "        self.batch_size = 64\n",
    "        self.lstm_layers = 3\n",
    "        self.num_epochs = 10\n",
    "        self.train_window = 192     #24 datapoints per day * 7 days\n",
    "        self.test_window = 192\n",
    "        self.test_predict_start = 168 \n",
    "        self.predict_steps = 24\n",
    "        self.num_class = 370\n",
    "        self.cov_dim = 4\n",
    "        self.lstm_hidden_dim = 40\n",
    "        self.embedding_dim = 20\n",
    "        self.sample_times = 200\n",
    "        self.lstm_dropout = 0.1\n",
    "        self.predict_batch = 256\n",
    "\n",
    "        # device: CPU or GPU\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training the model\n",
    "\n",
    "We will be training the model on the electricity dataset (https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014#, see the Readme file for downloading instructions). It consists os 370 time series with hourly granularity. We will be using time series of length 192 (7 days). During training, we will use 168 steps as conditioning range and 24 as forecasting range.\n",
    "\n",
    "\n",
    "Please, note that the code (publicly available at https://github.com/husnejahan/DeepAR-pytorch/tree/master) does not use a validation dataset. This implies that they are validating the performance on the test set (which is a very bad practice). The easiest fix would be to obtain a val_set from train_set and a val_loader from it, using that data partition when evaluating the model inside the training loop and using test_loader exclusively at the end of the training to evaluate the final model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "train_len: 389101\n",
      "building datasets from data/elect...\n",
      "test_len: 2590\n",
      "building datasets from data/elect...\n",
      "weights: tensor([4.5579e-08, 4.7663e-08, 4.8446e-08,  ..., 8.4827e-05, 8.4776e-05,\n",
      "        8.4272e-05], dtype=torch.float64)\n",
      "num samples (per epoch): 5000\n",
      "Loading complete.\n",
      "Model:\n",
      "Net(\n",
      "  (embedding): Embedding(370, 20)\n",
      "  (lstm): LSTM(25, 40, num_layers=3, dropout=0.1)\n",
      "  (relu): ReLU()\n",
      "  (distribution_mu): Linear(in_features=120, out_features=1, bias=True)\n",
      "  (distribution_presigma): Linear(in_features=120, out_features=1, bias=True)\n",
      "  (distribution_sigma): Softplus(beta=1.0, threshold=20.0)\n",
      ")\n",
      "Starting training for 10 epoch(s)...\n",
      "Begin training and evaluation\n",
      "Epoch 1/10\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]Epoch 1 Evaluation - ND: 0.8052, RMSE: 17950.9151\n",
      "Iteration 0 | train_loss: 1.4016\n",
      "100%|██████████| 79/79 [00:15<00:00,  5.12it/s]\n",
      "Epoch 1 Evaluation - ND: 0.2715, RMSE: 8077.7871\n",
      "- Found new best ND\n",
      "Current Best ND is: 0.27146\n",
      "Epoch 2/10\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]Epoch 2 Evaluation - ND: 0.2638, RMSE: 7769.9871\n",
      "Iteration 0 | train_loss: -0.0403\n",
      "100%|██████████| 79/79 [00:16<00:00,  4.89it/s]\n",
      "Epoch 2 Evaluation - ND: 0.1322, RMSE: 2681.8336\n",
      "- Found new best ND\n",
      "Current Best ND is: 0.13220\n",
      "Epoch 3/10\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]Epoch 3 Evaluation - ND: 0.1324, RMSE: 2667.3168\n",
      "Iteration 0 | train_loss: -0.4930\n",
      "100%|██████████| 79/79 [00:16<00:00,  4.82it/s]\n",
      "Epoch 3 Evaluation - ND: 0.1183, RMSE: 2316.9948\n",
      "- Found new best ND\n",
      "Current Best ND is: 0.11827\n",
      "Epoch 4/10\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]Epoch 4 Evaluation - ND: 0.1151, RMSE: 2240.2594\n",
      "Iteration 0 | train_loss: -0.6584\n",
      "100%|██████████| 79/79 [00:15<00:00,  4.94it/s]\n",
      "Epoch 4 Evaluation - ND: 0.1069, RMSE: 2119.5717\n",
      "- Found new best ND\n",
      "Current Best ND is: 0.10687\n",
      "Epoch 5/10\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]Epoch 5 Evaluation - ND: 0.1078, RMSE: 2120.7831\n",
      "Iteration 0 | train_loss: -0.8054\n",
      "100%|██████████| 79/79 [00:16<00:00,  4.72it/s]\n",
      "Epoch 5 Evaluation - ND: 0.1095, RMSE: 2187.1569\n",
      "Current Best ND is: 0.10687\n",
      "Epoch 6/10\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]Epoch 6 Evaluation - ND: 0.1287, RMSE: 2571.3336\n",
      "Iteration 0 | train_loss: -0.9260\n",
      "100%|██████████| 79/79 [00:16<00:00,  4.80it/s]\n",
      "Epoch 6 Evaluation - ND: 0.1362, RMSE: 3132.6756\n",
      "Current Best ND is: 0.10687\n",
      "Epoch 7/10\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]Epoch 7 Evaluation - ND: 0.1298, RMSE: 3002.0403\n",
      "Iteration 0 | train_loss: -0.8739\n",
      "100%|██████████| 79/79 [00:16<00:00,  4.93it/s]\n",
      "Epoch 7 Evaluation - ND: 0.1006, RMSE: 1971.3738\n",
      "- Found new best ND\n",
      "Current Best ND is: 0.10063\n",
      "Epoch 8/10\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]Epoch 8 Evaluation - ND: 0.0980, RMSE: 1959.5957\n",
      "Iteration 0 | train_loss: -0.9319\n",
      "100%|██████████| 79/79 [00:15<00:00,  5.01it/s]\n",
      "Epoch 8 Evaluation - ND: 0.1007, RMSE: 2001.7822\n",
      "Current Best ND is: 0.10063\n",
      "Epoch 9/10\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]Epoch 9 Evaluation - ND: 0.0973, RMSE: 2057.3669\n",
      "Iteration 0 | train_loss: -1.0129\n",
      "100%|██████████| 79/79 [00:16<00:00,  4.93it/s]\n",
      "Epoch 9 Evaluation - ND: 0.0983, RMSE: 2006.7024\n",
      "- Found new best ND\n",
      "Current Best ND is: 0.09834\n",
      "Epoch 10/10\n",
      "  0%|          | 0/79 [00:00<?, ?it/s]Epoch 10 Evaluation - ND: 0.1044, RMSE: 2263.1573\n",
      "Iteration 0 | train_loss: -1.0346\n",
      "100%|██████████| 79/79 [00:15<00:00,  5.01it/s]\n",
      "Epoch 10 Evaluation - ND: 0.1007, RMSE: 1986.9465\n",
      "Current Best ND is: 0.09834\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Replace these with your own paths/names if required\n",
    "args_dataset = \"elect\"\n",
    "data_folder = \"data\"\n",
    "model_name = \"experiments/base_model\"\n",
    "restore_file = None  # e.g. \"epoch_5\" if you want to resume\n",
    "relative_metrics = False\n",
    "sampling = False\n",
    "\n",
    "# 1) Load parameters from JSON\n",
    "model_dir = os.path.join(model_name)\n",
    "json_path = os.path.join(model_dir, 'params.json')\n",
    "params = Params()\n",
    "\n",
    "params.relative_metrics = relative_metrics\n",
    "params.sampling = sampling\n",
    "params.model_dir = model_dir\n",
    "params.plot_dir = os.path.join(model_dir, 'figures')\n",
    "\n",
    "# Create plot_dir if it doesn't exist\n",
    "os.makedirs(params.plot_dir, exist_ok=True)\n",
    "\n",
    "# 2) Prepare logging\n",
    "set_logger(os.path.join(model_dir, 'train.log'))\n",
    "\n",
    "# 3) Create Datasets and Loaders\n",
    "logging.info('Loading datasets...')\n",
    "data_dir = os.path.join(data_folder, args_dataset)\n",
    "\n",
    "train_set = TrainDataset(data_dir, args_dataset, params.num_class)\n",
    "test_set = TestDataset(data_dir, args_dataset, params.num_class)\n",
    "sampler = WeightedSampler(data_dir, args_dataset)\n",
    "#sampler = WeightedSampler(data_dir, args_dataset, max_samples=5000)\n",
    "train_loader = DataLoader(train_set, batch_size=params.batch_size, sampler=sampler, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=params.predict_batch, sampler=RandomSampler(test_set), num_workers=0)\n",
    "logging.info('Loading complete.')\n",
    "\n",
    "# 4) Instantiate Model\n",
    "model = Net(params).to(params.device)\n",
    "\n",
    "logging.info(f'Model:\\n{model}')\n",
    "\n",
    "# 5) Optimiser\n",
    "optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "# 6) Loss Function\n",
    "deepAR_loss = loss_fn\n",
    "\n",
    "# 7) Train\n",
    "logging.info(f'Starting training for {params.num_epochs} epoch(s)...')\n",
    "train_and_evaluate(model, train_loader, test_loader, optimizer, deepAR_loss, params, restore_file=restore_file)\n",
    "logging.info('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of training (when finishing the first epoch), the predictions of the model will look something like this:\n",
    "\n",
    "![title](notebook_images/0.png)\n",
    "\n",
    "At epoch 11:\n",
    "\n",
    "![title](notebook_images/11.png)\n",
    "\n",
    "With loss function (all iterations):\n",
    "\n",
    "![title](notebook_images/elect_loss_summary.png)\n",
    "\n",
    "And ND error (per epoch):\n",
    "\n",
    "![title](notebook_images/elect_ND_summary.png)\n",
    "\n",
    "To achieve performance comparable to the values reported in the paper, you should train for at least 20 epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
